{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Diagnostics Getting Started Guide",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadjadasghari/Colab/blob/main/Diagnostics_Getting_Started_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNkbpW7ouEf"
      },
      "source": [
        "  \n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/>\n",
        "  </td>\n",
        "\n",
        "----\n",
        "\n",
        "# Model Diagnostics\n",
        "\n",
        "\n",
        "Throughout the process of training your machine learning (ML) model, you may want to investigate your model's failures in order to understand which areas need improvement. Looking at an error analysis after each training iteration can help you understand whether you need to revise your annotations, make your ontology more clear, or create more training data that targets a specific area.\n",
        "Labelbox now offers a Model Diagnostics tool that analyzes the performance of your model's predictions in a single interface.\n",
        "With Model Diagnostics, you can:\n",
        "*   Inspect model behavior across experiments\n",
        "*   Adjust model hyperparameters and visualize model failures\n",
        "*   Use the Python SDK to create the analysis pipeline\n",
        "\n",
        "## How it works\n",
        "\n",
        "Configuring Model Diagnostics is all done via the SDK. We have created a Google colab notebook to demonstrate this process. The notebook also includes a section that leverages MAL in order to quickly create ground truth annotations.\n",
        "An Experiment is a specific instance of a model generating output in the form of predictions.\n",
        "In Labelbox, the `Model` object represents your ML model and it is what you'll be performing experiments on. It references a set of annotations specified by an ontology. \n",
        "The `Model Run` object represents the experiment itself. It is a specific instance of a `Model` with preconfigured hyperparameters (training data). You can upload inferences across each `Model Run`, filter by IoU score, and compare your model's predictions against the annotations from your training data.\n",
        "\n",
        "## Steps\n",
        "\n",
        "1. Have a set of ground truth labels in a project\n",
        "2. Install beta release of the SDK (SDK versions that are compatible with Model Diagnostics will have a \"b\" in the version name. The first SDK release to support this is 2.5b0.)\n",
        "3. Create a `Model`\n",
        "4. Create a `Model Run`\n",
        "5. Compute predictions\n",
        "6. Compute model performance metrics\n",
        "7. Upload labels, predictions, and metrics\n",
        "8. Navigate to the `Models` tab on Labelbox\n",
        "\n",
        "## Best practices\n",
        "Currently there is a limit of 2000 images per model run. We suggest uploading lower performing examples from your test set."
      ],
      "id": "EyNkbpW7ouEf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subsequent-magic"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Install dependencies"
      ],
      "id": "subsequent-magic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voluntary-minister",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2d050c-f198-41e4-a90d-c58cd3110578"
      },
      "source": [
        "!pip install labelbox==2.7b1 \\\n",
        "             requests \\\n",
        "             ndjson \\\n",
        "             scikit-image \\\n",
        "             PILLOW \\\n",
        "             tensorflow \\\n",
        "             opencv-python \\\n",
        "             base36"
      ],
      "id": "voluntary-minister",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting labelbox==2.7b1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/c6/9297bd242d51ea23d42445544a8c826016df8e7ea78f79381f73827f49e5/labelbox-2.7b1-py3-none-any.whl (79kB)\n",
            "\r\u001b[K     |████▏                           | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 20kB 30.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 30kB 22.2MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 40kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 51kB 16.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 61kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 71kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting ndjson\n",
            "  Downloading https://files.pythonhosted.org/packages/70/c9/04ba0056011ba96a58163ebfd666d8385300bd12da1afe661a5a147758d7/ndjson-0.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (0.16.2)\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Collecting base36\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/25/5ed7384b92e7c35909b8e939096cc24a13d64fc4e3a16c0d4b9438c3f88c/base36-0.1.1-py2.py3-none-any.whl\n",
            "Collecting pydantic<2.0,>=1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 23.8MB/s \n",
            "\u001b[?25hCollecting backoff==1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/32/c5dd4f4b0746e9ec05ace2a5045c1fc375ae67ee94355344ad6c7005fd87/backoff-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from labelbox==2.7b1) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labelbox==2.7b1) (1.19.5)\n",
            "Requirement already satisfied: google-api-core>=1.22.1 in /usr/local/lib/python3.7/dist-packages (from labelbox==2.7b1) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.1.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.5.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox==2.7b1) (57.0.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox==2.7b1) (20.9)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox==2.7b1) (1.31.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox==2.7b1) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox==2.7b1) (1.53.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core>=1.22.1->labelbox==2.7b1) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core>=1.22.1->labelbox==2.7b1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core>=1.22.1->labelbox==2.7b1) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=1.21.1->google-api-core>=1.22.1->labelbox==2.7b1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
            "Installing collected packages: pydantic, backoff, ndjson, base36, labelbox\n",
            "Successfully installed backoff-1.10.0 base36-0.1.1 labelbox-2.7b1 ndjson-0.3.1 pydantic-1.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wooden-worship",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1b41ee-9c20-494a-c3ec-e8ef7c16cd50"
      },
      "source": [
        "# Run these if running in a colab notebook\n",
        "COLAB = \"google.colab\" in str(get_ipython())\n",
        "\n",
        "if COLAB:\n",
        "    !git clone https://github.com/Labelbox/labelbox-python.git\n",
        "    !cd labelbox-python && git checkout mea-dev\n",
        "    !mv labelbox-python/examples/model_assisted_labeling/*.py .\n",
        "    !mv labelbox-python/examples/model_assisted_labeling/mapillary_sample.csv ."
      ],
      "id": "wooden-worship",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'labelbox-python'...\n",
            "remote: Enumerating objects: 4812, done.\u001b[K\n",
            "remote: Counting objects: 100% (1714/1714), done.\u001b[K\n",
            "remote: Compressing objects: 100% (544/544), done.\u001b[K\n",
            "remote: Total 4812 (delta 1181), reused 1623 (delta 1148), pack-reused 3098\u001b[K\n",
            "Receiving objects: 100% (4812/4812), 74.32 MiB | 25.72 MiB/s, done.\n",
            "Resolving deltas: 100% (3224/3224), done.\n",
            "Branch 'mea-dev' set up to track remote branch 'mea-dev' from 'origin'.\n",
            "Switched to a new branch 'mea-dev'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "latter-leone"
      },
      "source": [
        "Import libraries"
      ],
      "id": "latter-leone"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "committed-richards"
      },
      "source": [
        "from io import BytesIO\n",
        "from getpass import getpass\n",
        "import uuid\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from tqdm import notebook\n",
        "from collections import defaultdict\n",
        "import ndjson\n",
        "import os\n",
        "\n",
        "from labelbox.schema.ontology import OntologyBuilder, Tool\n",
        "from labelbox import Client, LabelingFrontend, MALPredictionImport, DataRow\n",
        "from labelbox.data.metrics.iou import datarow_miou\n",
        "\n",
        "try:\n",
        "    from image_model import predict, load_model, class_mappings\n",
        "    from ndjson_utils import (\n",
        "        create_boxes_ndjson, \n",
        "        create_polygon_ndjson, \n",
        "        create_mask_ndjson, \n",
        "        create_point_ndjson\n",
        "    )\n",
        "except ModuleNotFoundError: \n",
        "    # !git clone https://github.com/Labelbox/labelbox-python.git\n",
        "    # !cd labelbox-python && git checkout mea-dev\n",
        "    # !mv labelbox-python/examples/model_assisted_labeling/*.py .\n",
        "    # !mv labelbox-python/examples/model_assisted_labeling/mapillary_sample.csv .\n",
        "    raise Exception(\"You will need to run from the labelbox-python git repo\")"
      ],
      "id": "committed-richards",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alternate-promotion"
      },
      "source": [
        "Configure client"
      ],
      "id": "alternate-promotion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "economic-chase"
      },
      "source": [
        "API_KEY = None\n",
        "PROJECT_NAME = \"Diagnostics Demo\"\n",
        "MODEL_NAME = \"MSCOCO-Mapillary\"\n",
        "MODEL_VERSION = \"0.0.0\""
      ],
      "id": "economic-chase",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affecting-myanmar"
      },
      "source": [
        "client = Client(api_key=API_KEY)\n",
        "load_model() # initialize Tensorflow Model"
      ],
      "id": "affecting-myanmar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blessed-venture"
      },
      "source": [
        "## Setup a project"
      ],
      "id": "blessed-venture"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuOt_dgEjCyD"
      },
      "source": [
        "class_mappings = {\n",
        "    1: {\"name\": 'person', \"kind\": \"bbox\"},\n",
        "    2: {\"name\": 'bicycle', \"kind\": \"segmentation\"},\n",
        "    3: {\"name\": 'car', \"kind\": \"bbox\"},\n",
        "    4: {\"name\": 'motorcycle', \"kind\": \"bbox\"},\n",
        "    6: {\"name\": 'bus', \"kind\": \"polygon\"},\n",
        "    7: {\"name\": 'train', \"kind\": \"polygon\"},\n",
        "    8: {\"name\": 'truck', \"kind\": \"polygon\"},\n",
        "    10: {\"name\": 'traffic light', \"kind\": \"point\"},\n",
        "    11: {\"name\": 'fire hydrant', \"kind\": \"bbox\"},\n",
        "    13: {\"name\": 'stop sign', \"kind\": \"segmentation\"},\n",
        "    14: {\"name\": 'parking meter', \"kind\": \"point\"},\n",
        "}"
      ],
      "id": "XuOt_dgEjCyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "modern-program",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "6b81edf3-2218-4390-ae72-7566210c0976"
      },
      "source": [
        "print(f\"Setting up: {PROJECT_NAME}\")\n",
        "\n",
        "# --- setup ontology\n",
        "tools = []\n",
        "for target in class_mappings.values():\n",
        "    if target[\"kind\"] == \"bbox\":\n",
        "        tool = Tool.Type.BBOX\n",
        "    elif target[\"kind\"] == \"polygon\":\n",
        "        tool = Tool.Type.POLYGON\n",
        "    elif target[\"kind\"] == \"segmentation\":\n",
        "        tool = Tool.Type.SEGMENTATION\n",
        "    elif target[\"kind\"] == \"point\":\n",
        "        tool = Tool.Type.POINT\n",
        "    else:\n",
        "        raise ValueError(\"Type not supported\")\n",
        "\n",
        "    tools.append(Tool(tool=tool, name=target[\"name\"]))\n",
        "\n",
        "ontology_builder = OntologyBuilder(tools=tools)\n",
        "\n",
        "# --- setup project\n",
        "project = client.create_project(name=PROJECT_NAME)\n",
        "editor = next(client.get_labeling_frontends(where=LabelingFrontend.name == \"Editor\"))\n",
        "project.setup(editor, ontology_builder.asdict())\n",
        "\n",
        "# --- setup dataset\n",
        "# load mapillary sample\n",
        "with open('mapillary_sample.csv', 'r') as file:\n",
        "  rows = [row[:-1].split(\",\") for row in file.readlines()]\n",
        "  data_rows = [{DataRow.row_data: row[0], DataRow.external_id: row[1]} for row in rows]\n",
        "\n",
        "dataset = client.create_dataset(name=\"Mapillary Diagnostics Demo\")\n",
        "task = dataset.create_data_rows(data_rows)\n",
        "task.wait_till_done()\n",
        "print(f\"Dataset Created: {dataset.uid}\")\n",
        "\n",
        "project.datasets.connect(dataset)\n",
        "project_id = project.uid\n",
        "\n",
        "ontology = project.ontology()\n",
        "schema_lookup = {tool.name: tool for tool in ontology.tools()}"
      ],
      "id": "modern-program",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up: Diagnostics Demo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2cc955d3bcf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# --- setup project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_project\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROJECT_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0meditor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labeling_frontends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLabelingFrontend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Editor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meditor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0montology_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dated-burden"
      },
      "source": [
        "## Create Predictions\n",
        "* Loop over data_rows, make predictions, and create ndjson"
      ],
      "id": "dated-burden"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7wo1cUaBiNm"
      },
      "source": [
        "RUN_MAL = True\n",
        "\n",
        "if RUN_MAL:\n",
        "    datarows = project.export_queued_data_rows()\n",
        "    project.enable_model_assisted_labeling()"
      ],
      "id": "z7wo1cUaBiNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asian-savings"
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for datarow in notebook.tqdm(datarows):\n",
        "    np_image_bytes = np.array([requests.get(datarow[\"rowData\"]).content])\n",
        "    datarow_id = datarow[\"id\"]\n",
        "    w, h = Image.open(BytesIO(np_image_bytes[0])).size\n",
        "\n",
        "    \n",
        "    prediction = predict(np_image_bytes, min_score=0.25, height=h, width=w)\n",
        "    \n",
        "    boxes, classes, seg_masks = prediction[\"boxes\"], prediction[\"class_indices\"], prediction[\"seg_masks\"]\n",
        "    for box, class_idx, seg in zip(boxes, classes, seg_masks):\n",
        "        \n",
        "        if class_idx in class_mappings:\n",
        "            \n",
        "            class_name = class_mappings.get(class_idx)\n",
        "            schema = schema_lookup.get(class_name[\"name\"], None)\n",
        "            schema_id = schema.feature_schema_id\n",
        "\n",
        "            if schema.tool == Tool.Type.POLYGON:\n",
        "                predictions.append(\n",
        "                    create_polygon_ndjson(datarow_id, schema_id, seg)\n",
        "                    )\n",
        "            elif schema.tool == Tool.Type.BBOX:\n",
        "                predictions.append(\n",
        "                    create_boxes_ndjson(datarow_id, schema_id, *box)\n",
        "                    )\n",
        "            elif schema.tool.name == Tool.Type.POINT:\n",
        "                predictions.append(\n",
        "                    create_point_ndjson(datarow_id, schema_id, *box)\n",
        "                )\n",
        "            elif schema.tool.name == Tool.Type.SEGMENTATION:\n",
        "                predictions.append(\n",
        "                    create_mask_ndjson(client, datarow_id, schema_id, seg, (255, 0, 0))\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError"
      ],
      "id": "asian-savings",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "perfect-seafood"
      },
      "source": [
        "## **Optional** - Create labels with Model Assisted Labeling\n",
        "\n",
        "* Pre-label image so that we can quickly create ground truth\n",
        "* Create ground truth data for Model Diagnostics\n",
        "* Click on link below to label"
      ],
      "id": "perfect-seafood"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "subject-painting"
      },
      "source": [
        "if RUN_MAL:\n",
        "    upload_task = MALPredictionImport.create_from_objects(client, project.uid, f'mal-import-{uuid.uuid4()}', predictions)\n",
        "    upload_task.wait_until_done()\n",
        "    print(upload_task.state , '\\n')"
      ],
      "id": "subject-painting",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV4U1W4H_eMq"
      },
      "source": [
        "print(f\"https://app.labelbox.com/go-label/{project.uid}\")"
      ],
      "id": "MV4U1W4H_eMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stopped-mandate"
      },
      "source": [
        "## Export Labels\n",
        "\n",
        "We do not support `Skipped` labels and have a limit of **2000**"
      ],
      "id": "stopped-mandate"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "excited-seminar"
      },
      "source": [
        "MAX_LABELS = 2000\n",
        "labels = [l for l in requests.get(project.export_labels()).json() if l[\"Label\"]][:MAX_LABELS]"
      ],
      "id": "excited-seminar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smoking-catering"
      },
      "source": [
        "## Setup Model & Model Run"
      ],
      "id": "smoking-catering"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mental-minnesota"
      },
      "source": [
        "lb_model = client.create_model(name = MODEL_NAME, ontology_id = project.ontology().uid)\n",
        "lb_model_run = lb_model.create_model_run(MODEL_VERSION)"
      ],
      "id": "mental-minnesota",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu8h6h0g-Fe2"
      },
      "source": [
        "Select label ids to upload"
      ],
      "id": "cu8h6h0g-Fe2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "static-coordinate"
      },
      "source": [
        "lb_model_run.upsert_labels([label['ID'] for label in labels])"
      ],
      "id": "static-coordinate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_u1ak2n7qn5"
      },
      "source": [
        "### Compute Metrics"
      ],
      "id": "g_u1ak2n7qn5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "committed-fairy"
      },
      "source": [
        "# Note that the `datarow_miou` function downloads segmentation masks\n",
        "# For large projects this logic should be wrapped in a threadpool for faster execution.\n",
        "\n",
        "metric_annotations = []\n",
        "grouped_predictions = defaultdict(list)\n",
        "labeled_datarows = {label[\"DataRow ID\"] for label in labels}\n",
        "labeled_predictions = [pred for pred in predictions if pred['dataRow']['id'] in labeled_datarows] # filter to datarows with uploaded labels\n",
        "\n",
        "for prediction in labeled_predictions:\n",
        "    grouped_predictions[prediction['dataRow']['id']].append(prediction)\n",
        "    \n",
        "for label in labels:\n",
        "    datarow_id = label['DataRow ID']\n",
        "    # ignore skipped\n",
        "    if len(grouped_predictions[datarow_id]):\n",
        "        score = datarow_miou(label, grouped_predictions[datarow_id])\n",
        "        if score is None:\n",
        "            continue\n",
        "        \n",
        "        metric_annotations.append(  {\n",
        "            \"uuid\" : str(uuid.uuid4()),\n",
        "            \"dataRow\" : {\n",
        "                \"id\": datarow_id,\n",
        "            },\n",
        "            \"metricValue\" : score\n",
        "            }\n",
        "        )"
      ],
      "id": "committed-fairy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anonymous-addition"
      },
      "source": [
        "upload_task = lb_model_run.add_predictions(f'diagnostics-import-{uuid.uuid4()}', labeled_predictions + metric_annotations)\n",
        "upload_task.wait_until_done()\n",
        "print(upload_task.state)"
      ],
      "id": "anonymous-addition",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTjGOyIW-3op"
      },
      "source": [
        "### Open Model Run"
      ],
      "id": "uTjGOyIW-3op"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrll9K6Q9tGK"
      },
      "source": [
        "for idx, annotation_group in enumerate(lb_model_run.annotation_groups()):\n",
        "    if idx == 5:\n",
        "        break\n",
        "    print(annotation_group.url)"
      ],
      "id": "zrll9K6Q9tGK",
      "execution_count": null,
      "outputs": []
    }
  ]
}